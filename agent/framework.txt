Agent，我们启动**'autowzry-agent'**项目的框架设计阶段。
核心项目目标 (Goal): 基于强化学习 (RL) 技术，构建一个用于 MOBA 游戏策略的自主决策 Agent。
当前指令 (Instruction): 暂停所有代码开发工作。我们首要任务是完成项目的系统架构 (System Architecture) 设计和工程蓝图 (Engineering Blueprint) 的制定。
任务焦点 (Focus Areas):
    模块化设计 (Modularity): 确定 Agent 的核心功能模块。请提出一个完备的模块列表，并定义每个模块的最小功能集 (Minimal Feature Set)。
    职责边界 (Responsibility Scope): 明确界定每个模块的输入、输出和操作职责。讨论具体功能或函数应归属于哪个模块，以确保高内聚、低耦合。
    结构完备性 (Completeness Check): 评估当前设想的模块集合是否完备（覆盖所有功能需求），是否需要增删。
    文件结构规范 (File Structure): 确定初期项目所需的文件数量和组织层次。
我们现在将逐一探讨这些点，从核心模块的初步划分开始。你可以在framework.txt中看到我设计的模块化方案. 你的任务是提供结构化的反馈和优化建议。



项目: autowzry-agent

前期采用的技术:
* autowzry, 我自己开发的python项目, 可以采用adb连接安卓设备, 进行设备开机，启动王者荣耀游戏，进入对战状态，移动英雄，释放技能，对战状态判断等功能，目前所有的对战策略都是随机移动和攻击，缺乏有效的模型去指挥英雄操作。
* autowzry-lite, 精简的autowzry, 只进行截图对战状态判断和英雄操作, 已经足够用于对战过程训练了。
* 纯Pytorch机器学习框架。我希望基于Pytorch去完成强化学习代码框架的开发。框架如下，可以看到我并没有特定的游戏（即王者荣耀）开发，与王者荣耀相关的部分在兼容模块里面，其他的模块我都尽量写的通用和框架形式。这样方便其他用户移植到不同的游戏中，或者切换不同的卷积模型。
* 基于opencv和numpy等基础的库开发, 暂时不要借助其他模型进行辅助识别，会增加代码逻辑



* 工具函数库或模块 tool/各自函数, 
* * 如读取视频,返回游戏画面
* * 根据两帧图像预测移动方向 other操作, 逻辑生成, 例如在水晶就, 攻击=0，不在水晶攻击=N，简单的先生成ACTION[index]
* * 自定义ACTION函数(图1,图2) 判断移动方向. 生成 ACTION[移动, other], other操作, 逻辑生成, 例如在水晶就, 攻击=0，不在水晶攻击=N，简

* 状态&评分函数库：包含对当前状态的识别(opencv或者特定模型(能不用尽量不用))，不同状态的分数词典(如默认+0.01,击杀+5,死亡-2,助攻1),前期识别先适配少的,后面再基于root或者其他方式识别游戏状态.官方肯定是用api直接查的

* 模型模块, 调试初期,可以写一个简单的模型，甚至是线性层，基础模块, 后面基于此再封装 init(输入图像,中间参数,输出维度)

* 动作模块(启用哪些操作,移动=兼容模块.移动,...)，包含操作词典{index:index, 操作:'移动', 方向:s}. 根据<启用哪些操作>确定ACTION[index]=['移动','攻击','信号'],以及映射index=0~N对应的操作. 

* 数据集构建模块: 深度耦合于动作模块、模型模块
* * 1.0 init(结束函数, 模型M/自定义ACTION函数, 动作, 读取图像, 读取的频率, 存储数据方式)| 读取图像(args)==读取adb画面方法, 后面可视情况修改为读入视频方法, 在工具模块封装一个读取视频方法(第n帧)等
* * 2.0 收集一次数据():
* * 2.1 if self.模型M == None: 读取2页画面, ACTION = 自定义ACTION函数(图1,图2). 封装[图1,图2,ACTION]
* * 2.2 if self.模型M != None: 读取1页画面, Mat = M(图), ACTION=Mat[index,max_val_pos], 动作(ACTION), 读取下一页照片. 封装[图1,图2,ACTION]
* * 2.3 可选, 第2页画面与第1页间隔 > N s,以免死亡错过判断 
* * 3.0 收集N次数据(): data = []; data.append(收集一次数据);  while self.结束函数 != None: data.append(vs);
* * 4.0 RUN(停止函数), 使用模型进行操作, 使用模型的接口. while not 停止函数: 收集一次数据()
* * 5.0 数据集的存储方案? 960x540(1.5MB),
* * 5.1 存储到?类型的文件
* * 5.2 数据集加载方案


* 兼容模块(亦可叫设备模块，API模块，中转模块)
* * 1.0 底层及设备初始化, 如 try: import autowzry; else import autowzry-lite as autowzry. 
* * 2.0 封装动作模块(对动作模块的常用函数进行结束): 不同设备的控制手段不同需要封装, 
* * 2.1 self.移动(angle=0,time=1), self.攻击(type=0,angle=-1), self.信号(type)
* * 3.0 封装截图函数 不同设备的截图手段
* * 3.1 读取视频、文件夹中的所有视频返回图片
* * 3.2 adb/win接口截图
* * 3.2 对图像进行基础的resize或者其他预处理
* * 4.0 停止收集数据函数, 设备画面判断, 当游戏在运行时(挂机/对战)，可以用autowzry.判断对战中()，若是读取视频(观战)，则需要封装判断函数, 这种方式页兼容挂机/对战.
* * 5.0 封装进入对战页面()
* * 5.1 `进入对战页面()`, 存在autowzry环境时, 可以自动进入. 若只有精简的autowzry-lite, 可以采用手动进入对战页面或者打开视频的操作。autowzry-lite,足够用于训练过程的截图和操作.

* 强化学习主模块agent
* * 1.0 初始化: 兼容模块、动作模块(设备)、模型M(输入图片维度, 动作模块.维度index, 每个操作.维度s)、读取图像方法(设备)、数据集模块(模型,动作,读取图像,...)、
* * 2.0 收集数据集阶段()
* * 2.1 挂机模式, 进入对战页面(), sleep(40s), 开始收集数据(模型=None,读取图像=snapshop)
* * 2.2 对战模式, 进入对战页面(), 开始收集数据(模型=M,读取图像=snapshop)
* * 2.3 观战模式, 开始收集数据(模型=M,读取图像=工具库.读取视频())
* * 3.0 训练模型阶段()：加载数据[图1,图2,ACTION]、初始化评分函数、模型M,N
* * 3.1 构建batch和训练, DATA1[batch,图1], Mat1=M(DATA1),  DATA2[batch,图2], Mat2=M(DATA2)
* * 3.2 Q1=Mat[Action], R=评分(图2), Mat2=N(图2), Q2=Mat2[:,max_val_pos], Loss(Q1.sum(),R+\gamma * Q2.sum()). 注意Action可能是观战获得, 或者模型不佳, 不是Q1最大的位置.
* * 3.2 根据Loss, 梯度优化模型M
* * 3.3 优化N伦, 还是损失函数小于阈值后, 备份模型N, 并替换模型N=M
* * 4.0 应用阶段
* * 4.1 进入对战页面(), 数据集模块.RUN(停止函数)
